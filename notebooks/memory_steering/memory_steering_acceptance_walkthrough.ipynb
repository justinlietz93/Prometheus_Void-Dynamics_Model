{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memory steering \u00b7 Acceptance walkthrough\n\nThis notebook exercises a subset of the checks from `code/memory_steering/memory_steering_acceptance.py`. We run the step response and noise-suppression experiments on a short horizon and inspect their acceptance flags.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def _ensure_repo_on_path():\n",
        "    cwd = Path.cwd().resolve()\n",
        "    candidates = [cwd, cwd.parent, cwd.parent.parent]\n",
        "    for candidate in candidates:\n",
        "        utils_py = candidate / 'notebooks' / 'utils.py'\n",
        "        if utils_py.exists():\n",
        "            sys.path.insert(0, str(candidate))\n",
        "            return candidate\n",
        "    return cwd\n",
        "\n",
        "_ensure_repo_on_path()\n",
        "\n",
        "from notebooks.utils import ensure_repo_path, allocate_artifacts, preview_json\n",
        "\n",
        "repo_root = ensure_repo_path()\n",
        "print(f\"Using repo root: {repo_root}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from code.memory_steering.memory_steering_acceptance import (\n",
        "except ModuleNotFoundError:\n",
        "    from notebooks.utils import ensure_repo_path as _vdm_ensure_repo_path\n",
        "    _vdm_ensure_repo_path()\n",
        "    import importlib\n",
        "    importlib.invalidate_caches()\n",
        "    from code.memory_steering.memory_steering_acceptance import (\n",
        "\n",
        "    step_response_experiment,\n",
        "    noise_suppression_experiment,\n",
        ")\n",
        "\n",
        "step = step_response_experiment(seed=0, steps=96, g=0.12, lam=0.08)\n",
        "noise = noise_suppression_experiment(seed=0, steps=128, g=0.12, lam=0.08, noise_std=0.05)\n",
        "print(preview_json({\n",
        "    \"step_passes\": {k: step[k] for k in (\"pass_pole\", \"pass_Mstar\", \"pass_overshoot\")},\n",
        "    \"noise_delta_snr_db\": noise[\"delta_snr_db\"],\n",
        "    \"noise_pass_snr\": noise[\"pass_snr\"],\n",
        "}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "artifacts = allocate_artifacts(\"memory_steering\", \"acceptance_notebook_demo\")\n",
        "print(preview_json({name: str(path) for name, path in artifacts.items()}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook checklist\n\n* Each helper writes its own figure (stored alongside the scripted harness outputs).\n* The reduced step counts keep the demo quick; raise them to match the CLI defaults for full acceptance runs.\n* Inspect `noise['delta_snr_db']` to ensure the filter improves SNR before trusting production settings.\n* Layer additional experiments (boundedness, Lyapunov, reproducibility) using the same call pattern.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}